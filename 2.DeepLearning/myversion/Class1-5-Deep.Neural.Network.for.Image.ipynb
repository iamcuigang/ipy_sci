{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, \n",
    "                    learning_rate=0.0075, num_iterations=3000, \n",
    "                    print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    ### Init parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in xrange(0, num_iterations):\n",
    "        ### Forward propagation\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \n",
    "                                               activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \n",
    "                                               activation='sigmoid')\n",
    "        ### Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        ### Init backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        ### Backward propagation\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2,\n",
    "                                         activation='sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1,\n",
    "                                         activation='relu')\n",
    "        ### Set grads\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        ### Update parameters\n",
    "        parameters = update_parameters(parameters, grads, \n",
    "                                       learning_rate)\n",
    "        ### Retrieve parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        ### Print the cost\n",
    "        if print_cost and i%100 == 0:\n",
    "            print 'Cost iter %d, %f'%(i, np.squeeze(cost))\n",
    "            costs.append(cost)\n",
    "    # plot the cost\n",
    "#     plt.plot(np.squeeze(costs))\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.xlabel('iterations (per tens)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "    \n",
    "    return parameters        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, \n",
    "                  learning_rate=0.0075, num_iterations=3000, \n",
    "                  print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    for i in xrange(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, \n",
    "                                       learning_rate)\n",
    "        ### Print the cost\n",
    "        if print_cost and i%100 == 0:\n",
    "            print 'Cost iter %d, %f'%(i, np.squeeze(cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "#     plt.plot(np.squeeze(costs))\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.xlabel('iterations (per tens)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "    \n",
    "    return parameters            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
